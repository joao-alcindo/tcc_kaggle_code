{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\joaoa\\.conda\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "#assert timm.__version__ == \"0.3.2\"  # version check\n",
    "import timm.optim.optim_factory as optim_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import util.misc as misc\n",
    "from util.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "from util.custom_dataset import CustomDataset\n",
    "import util.transform_npy as transform_npy\n",
    "\n",
    "\n",
    "import models_mae\n",
    "\n",
    "from engine_pretrain import train_one_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configurar o diretório de logs e salvamento de modelos\n",
    "log_dir = 'D:/dados_tcc/output_dir/logs'  # Substitua 'logs' pelo diretório desejado\n",
    "output_dir = 'D:/dados_tcc/output_dir/models'  # Substitua 'saved_models' pelo diretório desejado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir hiperparâmetros\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "input_size = 224\n",
    "epochs = 50\n",
    "start_epoch = 0\n",
    "num_workers = 2\n",
    "\n",
    "resume_path = ''\n",
    "\n",
    "pin_mem = True\n",
    "\n",
    "\n",
    "seed = 42\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('train_one_epoch-args', add_help=False)\n",
    "\n",
    "    parser.add_argument('--accum_iter', default=1, type=int,\n",
    "                        help='Accumulate gradient iterations (for increasing the effective batch size under memory constraints)')\n",
    "    \n",
    "    parser.add_argument('--epochs', default=100, type=int)\n",
    "    \n",
    "    parser.add_argument('--mask_ratio', default=0.75, type=float,\n",
    "                        help='Masking ratio (percentage of removed patches).')\n",
    "\n",
    "\n",
    "    parser.add_argument('--warmup_epochs', type=int, default=5, metavar='N',\n",
    "                        help='epochs to warmup LR')\n",
    "    \n",
    "    parser.add_argument('--lr', type=float, default=None, metavar='LR',\n",
    "                        help='learning rate (absolute lr)')\n",
    "    parser.add_argument('--min_lr', type=float, default=0., metavar='LR',\n",
    "                        help='lower lr bound for cyclic schedulers that hit 0')\n",
    "    \n",
    "    return parser\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "args_train_epoch = get_args_parser()\n",
    "args_train_epoch = args_train_epoch.parse_args([])\n",
    "\n",
    "args_train_epoch.epochs = epochs\n",
    "args_train_epoch.lr = learning_rate\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/dados_tcc\"\n",
    "\n",
    "# Set the device for training (e.g., 'cuda' or 'cpu')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Fix the random seed for reproducibility\n",
    "seed = seed\n",
    "torch.manual_seed(seed)  # Set PyTorch random seed\n",
    "np.random.seed(seed)  # Set NumPy random seed\n",
    "\n",
    "cudnn.benchmark = True  # Enable CuDNN benchmark mode for optimized performance\n",
    "\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "log_writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# Carregar dados\n",
    "# Define the transformation\n",
    "transform_train = transforms.Compose([\n",
    "    transform_npy.AddingPad((512,512)),\n",
    "    transform_npy.ResizeNumpy((input_size, input_size)),\n",
    "    transform_npy.RandomHorizontalFlipNpy(),\n",
    "    transform_npy.RandomRotationNpy(degrees=(-30, 30)),  # Random rotation between -15 and 15 degrees\n",
    "    transforms.Lambda(lambda data: data.copy()),  # Copy the data to avoid grad error\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[130.10511327778235], std=[316.09062860899644])\n",
    "])\n",
    "\n",
    "    \n",
    "# Create a training dataset using the defined transformations\n",
    "train_dataset = CustomDataset(data_path=os.path.join(data_path, 'train'), transform=transform_train)\n",
    "\n",
    "\n",
    "sampler_train = torch.utils.data.RandomSampler(train_dataset)\n",
    "\n",
    "# Crie um DataLoader personalizado\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create data loader for training dataset\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size= batch_size,\n",
    "    pin_memory= pin_mem,\n",
    "    drop_last = True,\n",
    "    shuffle=True\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir o modelo (incluindo a definição da classe MaskedAutoencoderViT)\n",
    "\n",
    "# Criar uma instância do modelo e movê-lo para a GPU\n",
    "model = models_mae.MaskedAutoencoderViT(\n",
    "        img_size = input_size, patch_size=16, in_chans=1, embed_dim=768, depth=6, num_heads=6,\n",
    "        decoder_embed_dim=512, decoder_depth=4, decoder_num_heads=8,\n",
    "        mlp_ratio=4, norm_layer= partial(nn.LayerNorm, eps=1e-6))\n",
    "\n",
    "model.to(device)\n",
    "model = model.float()\n",
    "model = DataParallel(model)  # Habilite o treinamento paralelo\n",
    "\n",
    "\n",
    "model_without_ddp = model.module  # Obtenha o modelo sem o wrapper DataParallel\n",
    "\n",
    "\n",
    "# Definir a função de perda e otimizador\n",
    "criterion = NativeScaler()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate, betas=(0.9, 0.95))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joaoa\\OneDrive - Fundacao Getulio Vargas - FGV\\TCC\\tcc\\final-project-cows\\model\\pre_training.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Atualize a string exibida na barra de progresso\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m progress_bar\u001b[39m.\u001b[39mset_postfix(loss\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Adicione a perda atual à lista de perdas da época\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m epoch_losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "# ...\n",
    "\n",
    "# Configurar o logger\n",
    "log_filename = os.path.join(log_dir, 'training_v0_224.log')\n",
    "logging.basicConfig(filename=log_filename, level=logging.INFO, format='%(asctime)s [%(levelname)s] - %(message)s')\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()  # Tempo de início da época\n",
    "    \n",
    "    logging.info(f'Época [{epoch+1}/{epochs}]')\n",
    "    \n",
    "    # Use tqdm para obter uma barra de progresso\n",
    "    progress_bar = tqdm(train_loader, desc=f'Época [{epoch+1}/{epochs}]', leave=False, dynamic_ncols=True)\n",
    "    \n",
    "    epoch_losses = []  # Lista para armazenar as perdas da época\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        images = batch.to(device).float()\n",
    "        \n",
    "        # Zero os gradientes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Passe para a frente\n",
    "        loss, _, _ = model(images)\n",
    "        \n",
    "        # Retropropagação e otimização\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Atualize a string exibida na barra de progresso\n",
    "        progress_bar.set_postfix(loss=f'{loss.item():.4f}')\n",
    "        \n",
    "        # Adicione a perda atual à lista de perdas da época\n",
    "        epoch_losses.append(loss.item())\n",
    "    \n",
    "    end_time = time.time()  # Tempo de término da época\n",
    "    epoch_time = end_time - start_time  # Tempo total da época\n",
    "    \n",
    "    # Calcule a média das perdas da época\n",
    "    avg_epoch_loss = sum(epoch_losses) / len(epoch_losses)\n",
    "    \n",
    "    # Registre a perda média da época no arquivo de log\n",
    "    logging.info(f'Tempo da Época [{epoch+1}/{epochs}]: {epoch_time:.2f} segundos - Perda Média: {avg_epoch_loss:.4f}')\n",
    "    \n",
    "    # Salve o modelo a cada 5 épocas\n",
    "    if (epoch) % 5 == 0:\n",
    "        model_checkpoint = os.path.join(output_dir, f'model_v0_224.pt')\n",
    "        torch.save(model.state_dict(), model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training for 10 epochs\n",
      "log_dir: D:/dados_tcc/output_dir/logs\n",
      "Epoch: [0]  [  0/552]  eta: 1:17:23  lr: 0.000000  loss: 2.2565 (2.2565)  time: 8.4125  data: 2.6350  max mem: 1452\n",
      "Epoch: [0]  [ 10/552]  eta: 0:44:04  lr: 0.000004  loss: 2.1298 (2.0922)  time: 4.8791  data: 0.7350  max mem: 1907\n",
      "Epoch: [0]  [ 20/552]  eta: 0:44:57  lr: 0.000007  loss: 1.7570 (1.8240)  time: 4.9029  data: 0.5318  max mem: 1907\n",
      "Epoch: [0]  [ 30/552]  eta: 0:42:14  lr: 0.000011  loss: 1.3481 (1.6289)  time: 4.8434  data: 0.5238  max mem: 1907\n",
      "Epoch: [0]  [ 40/552]  eta: 0:41:53  lr: 0.000014  loss: 1.0799 (1.4868)  time: 4.7418  data: 0.6080  max mem: 1907\n",
      "Epoch: [0]  [ 50/552]  eta: 0:41:58  lr: 0.000018  loss: 1.0107 (1.3983)  time: 5.2653  data: 0.6795  max mem: 1907\n",
      "Epoch: [0]  [ 60/552]  eta: 0:41:48  lr: 0.000022  loss: 0.9556 (1.3248)  time: 5.4826  data: 0.6752  max mem: 1907\n",
      "Epoch: [0]  [ 70/552]  eta: 0:41:22  lr: 0.000025  loss: 0.8937 (1.2603)  time: 5.4903  data: 0.7022  max mem: 1907\n",
      "Epoch: [0]  [ 80/552]  eta: 0:40:49  lr: 0.000029  loss: 0.8018 (1.1982)  time: 5.4674  data: 0.6983  max mem: 1907\n",
      "Epoch: [0]  [ 90/552]  eta: 0:40:15  lr: 0.000033  loss: 0.6635 (1.1333)  time: 5.5069  data: 0.7102  max mem: 1907\n",
      "Epoch: [0]  [100/552]  eta: 0:39:04  lr: 0.000036  loss: 0.5617 (1.0767)  time: 5.1814  data: 0.9545  max mem: 1907\n",
      "Epoch: [0]  [110/552]  eta: 0:38:49  lr: 0.000040  loss: 0.4995 (1.0276)  time: 5.4626  data: 1.4146  max mem: 1907\n",
      "Epoch: [0]  [120/552]  eta: 0:38:20  lr: 0.000043  loss: 0.4826 (0.9826)  time: 6.0259  data: 1.5352  max mem: 1907\n",
      "Epoch: [0]  [130/552]  eta: 0:37:50  lr: 0.000047  loss: 0.4528 (0.9423)  time: 5.9815  data: 1.3735  max mem: 1907\n",
      "Epoch: [0]  [140/552]  eta: 0:37:20  lr: 0.000051  loss: 0.4476 (0.9074)  time: 6.1189  data: 1.3611  max mem: 1907\n",
      "Epoch: [0]  [150/552]  eta: 0:36:44  lr: 0.000054  loss: 0.4270 (0.8762)  time: 6.1733  data: 1.3757  max mem: 1907\n",
      "Epoch: [0]  [160/552]  eta: 0:36:01  lr: 0.000058  loss: 0.4206 (0.8476)  time: 6.0484  data: 1.3517  max mem: 1907\n",
      "Epoch: [0]  [170/552]  eta: 0:35:16  lr: 0.000062  loss: 0.4215 (0.8229)  time: 5.9669  data: 1.3288  max mem: 1907\n",
      "Epoch: [0]  [180/552]  eta: 0:34:28  lr: 0.000065  loss: 0.4227 (0.8006)  time: 5.9437  data: 1.3055  max mem: 1907\n",
      "Epoch: [0]  [190/552]  eta: 0:33:41  lr: 0.000069  loss: 0.4156 (0.7808)  time: 5.9501  data: 1.3118  max mem: 1907\n",
      "Epoch: [0]  [200/552]  eta: 0:32:52  lr: 0.000072  loss: 0.4135 (0.7628)  time: 5.9752  data: 1.3371  max mem: 1907\n",
      "Epoch: [0]  [210/552]  eta: 0:32:02  lr: 0.000076  loss: 0.4040 (0.7459)  time: 5.9759  data: 1.3350  max mem: 1907\n",
      "Epoch: [0]  [220/552]  eta: 0:31:11  lr: 0.000080  loss: 0.4187 (0.7316)  time: 5.9761  data: 1.3358  max mem: 1907\n",
      "Epoch: [0]  [230/552]  eta: 0:30:19  lr: 0.000083  loss: 0.4111 (0.7175)  time: 5.9673  data: 1.3297  max mem: 1907\n",
      "Epoch: [0]  [240/552]  eta: 0:29:27  lr: 0.000087  loss: 0.3916 (0.7038)  time: 5.9614  data: 1.3230  max mem: 1907\n",
      "Epoch: [0]  [250/552]  eta: 0:28:33  lr: 0.000091  loss: 0.3909 (0.6916)  time: 5.9506  data: 1.3115  max mem: 1907\n",
      "Epoch: [0]  [260/552]  eta: 0:27:40  lr: 0.000094  loss: 0.3880 (0.6801)  time: 5.9528  data: 1.3140  max mem: 1907\n",
      "Epoch: [0]  [270/552]  eta: 0:26:46  lr: 0.000098  loss: 0.3690 (0.6681)  time: 5.9634  data: 1.3250  max mem: 1907\n",
      "Epoch: [0]  [280/552]  eta: 0:25:51  lr: 0.000101  loss: 0.3360 (0.6559)  time: 5.9532  data: 1.3151  max mem: 1907\n",
      "Epoch: [0]  [290/552]  eta: 0:24:56  lr: 0.000105  loss: 0.3131 (0.6439)  time: 5.9528  data: 1.3143  max mem: 1907\n",
      "Epoch: [0]  [300/552]  eta: 0:24:01  lr: 0.000109  loss: 0.3109 (0.6340)  time: 5.9485  data: 1.3094  max mem: 1907\n",
      "Epoch: [0]  [310/552]  eta: 0:23:06  lr: 0.000112  loss: 0.2922 (0.6227)  time: 5.9524  data: 1.3140  max mem: 1907\n",
      "Epoch: [0]  [320/552]  eta: 0:22:10  lr: 0.000116  loss: 0.2784 (0.6123)  time: 5.9756  data: 1.3365  max mem: 1907\n",
      "Epoch: [0]  [330/552]  eta: 0:21:15  lr: 0.000120  loss: 0.2765 (0.6023)  time: 5.9758  data: 1.3364  max mem: 1907\n",
      "Epoch: [0]  [340/552]  eta: 0:20:19  lr: 0.000123  loss: 0.2730 (0.5934)  time: 5.9755  data: 1.3368  max mem: 1907\n",
      "Epoch: [0]  [350/552]  eta: 0:19:22  lr: 0.000127  loss: 0.2730 (0.5844)  time: 5.9752  data: 1.3368  max mem: 1907\n",
      "Epoch: [0]  [360/552]  eta: 0:18:26  lr: 0.000130  loss: 0.2722 (0.5770)  time: 5.9774  data: 1.3396  max mem: 1907\n",
      "Epoch: [0]  [370/552]  eta: 0:17:29  lr: 0.000134  loss: 0.2643 (0.5688)  time: 5.9786  data: 1.3406  max mem: 1907\n",
      "Epoch: [0]  [380/552]  eta: 0:16:33  lr: 0.000138  loss: 0.2650 (0.5613)  time: 6.0224  data: 1.3720  max mem: 1907\n",
      "Epoch: [0]  [390/552]  eta: 0:15:36  lr: 0.000141  loss: 0.2634 (0.5534)  time: 6.0391  data: 1.3885  max mem: 1907\n",
      "Epoch: [0]  [400/552]  eta: 0:14:39  lr: 0.000145  loss: 0.2552 (0.5461)  time: 5.9963  data: 1.3571  max mem: 1907\n",
      "Epoch: [0]  [410/552]  eta: 0:13:42  lr: 0.000149  loss: 0.2579 (0.5395)  time: 5.9816  data: 1.3419  max mem: 1907\n",
      "Epoch: [0]  [420/552]  eta: 0:12:45  lr: 0.000152  loss: 0.2640 (0.5329)  time: 5.9811  data: 1.3426  max mem: 1907\n",
      "Epoch: [0]  [430/552]  eta: 0:11:47  lr: 0.000156  loss: 0.2596 (0.5265)  time: 5.9781  data: 1.3404  max mem: 1907\n",
      "Epoch: [0]  [440/552]  eta: 0:10:50  lr: 0.000159  loss: 0.2596 (0.5217)  time: 5.9741  data: 1.3357  max mem: 1907\n",
      "Epoch: [0]  [450/552]  eta: 0:09:52  lr: 0.000163  loss: 0.2581 (0.5158)  time: 5.9769  data: 1.3381  max mem: 1907\n",
      "Epoch: [0]  [460/552]  eta: 0:08:54  lr: 0.000167  loss: 0.2532 (0.5107)  time: 5.9747  data: 1.3361  max mem: 1907\n",
      "Epoch: [0]  [470/552]  eta: 0:07:56  lr: 0.000170  loss: 0.2457 (0.5050)  time: 5.9700  data: 1.3303  max mem: 1907\n",
      "Epoch: [0]  [480/552]  eta: 0:06:58  lr: 0.000174  loss: 0.2472 (0.5003)  time: 5.9538  data: 1.3144  max mem: 1907\n",
      "Epoch: [0]  [490/552]  eta: 0:06:00  lr: 0.000178  loss: 0.2526 (0.4951)  time: 5.9669  data: 1.3247  max mem: 1907\n",
      "Epoch: [0]  [500/552]  eta: 0:05:02  lr: 0.000181  loss: 0.2427 (0.4906)  time: 5.6917  data: 1.5084  max mem: 1907\n",
      "Epoch: [0]  [510/552]  eta: 0:04:04  lr: 0.000185  loss: 0.2414 (0.4860)  time: 5.7182  data: 1.5618  max mem: 1907\n",
      "Epoch: [0]  [520/552]  eta: 0:03:06  lr: 0.000188  loss: 0.2436 (0.4814)  time: 5.9056  data: 1.4139  max mem: 1907\n",
      "Epoch: [0]  [530/552]  eta: 0:02:07  lr: 0.000192  loss: 0.2383 (0.4769)  time: 5.7920  data: 1.5579  max mem: 1907\n",
      "Epoch: [0]  [540/552]  eta: 0:01:09  lr: 0.000196  loss: 0.2394 (0.4726)  time: 5.8482  data: 1.6035  max mem: 1907\n",
      "Epoch: [0]  [550/552]  eta: 0:00:11  lr: 0.000199  loss: 0.2331 (0.4682)  time: 5.9310  data: 1.4016  max mem: 1907\n",
      "Epoch: [0]  [551/552]  eta: 0:00:05  lr: 0.000200  loss: 0.2330 (0.4678)  time: 5.9256  data: 1.3644  max mem: 1907\n",
      "Epoch: [0] Total time: 0:53:33 (5.8214 s / it)\n",
      "Averaged stats: lr: 0.000200  loss: 0.2330 (0.4678)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\joaoa\\OneDrive - Fundacao Getulio Vargas - FGV\\TCC\\tcc\\final-project-cows\\model\\pre_training.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# Save model checkpoint and statistics periodically\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mif\u001b[39;00m output_dir \u001b[39mand\u001b[39;00m (epoch \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m epochs):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     misc\u001b[39m.\u001b[39;49msave_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         output_dir, model\u001b[39m=\u001b[39;49mmodel, model_without_ddp\u001b[39m=\u001b[39;49mmodel_without_ddp, optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m         loss_scaler\u001b[39m=\u001b[39;49mloss_scaler, epoch\u001b[39m=\u001b[39;49mepoch)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# Prepare log statistics for logging\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m log_stats \u001b[39m=\u001b[39m {\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain_\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m train_stats\u001b[39m.\u001b[39mitems()},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/joaoa/OneDrive%20-%20Fundacao%20Getulio%20Vargas%20-%20FGV/TCC/tcc/final-project-cows/model/pre_training.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m: epoch,}\n",
      "File \u001b[1;32mc:\\Users\\joaoa\\OneDrive - Fundacao Getulio Vargas - FGV\\TCC\\tcc\\final-project-cows\\model\\util\\misc.py:298\u001b[0m, in \u001b[0;36msave_model\u001b[1;34m(args, epoch, model, model_without_ddp, optimizer, loss_scaler)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msave_model\u001b[39m(args, epoch, model, model_without_ddp, optimizer, loss_scaler):\n\u001b[1;32m--> 298\u001b[0m     output_dir \u001b[39m=\u001b[39m Path(args\u001b[39m.\u001b[39;49moutput_dir)\n\u001b[0;32m    299\u001b[0m     epoch_name \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(epoch)\n\u001b[0;32m    300\u001b[0m     \u001b[39mif\u001b[39;00m loss_scaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'output_dir'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Load model checkpoint and optimizer state if available\n",
    "# misc.load_model_new(resume_path, \n",
    "#                     model_without_ddp=model_without_ddp, \n",
    "#                     optimizer=optimizer, \n",
    "#                     loss_scaler=loss_scaler)\n",
    "\n",
    "\n",
    "# # Start training loop for specified number of epochs\n",
    "# print(f\"Start training for {epochs} epochs\")\n",
    "# start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(start_epoch, epochs):\n",
    "    \n",
    "#     # Perform one epoch of training and get training statistics\n",
    "#     train_stats = train_one_epoch(\n",
    "#         model, \n",
    "#         data_loader_train,\n",
    "#         optimizer, \n",
    "#         device, \n",
    "#         epoch, \n",
    "#         loss_scaler,\n",
    "#         log_writer=log_writer,\n",
    "#         args= args_train_epoch\n",
    "#     )\n",
    "    \n",
    "#     # Save model checkpoint and statistics periodically\n",
    "#     if output_dir and (epoch % 5 == 0 or epoch + 1 == epochs):\n",
    "#         misc.save_model_new(\n",
    "#             output_dir, model=model, model_without_ddp=model_without_ddp, optimizer=optimizer,\n",
    "#             loss_scaler=loss_scaler, epoch=epoch)\n",
    "    \n",
    "#     # Prepare log statistics for logging\n",
    "#     log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "#                     'epoch': epoch,}\n",
    "    \n",
    "#     # Write log statistics to file if applicable\n",
    "#     if output_dir and misc.is_main_process():\n",
    "#         if log_writer is not None:\n",
    "#             log_writer.flush()\n",
    "#         with open(os.path.join(output_dir, \"log.txt\"), mode=\"a\", encoding=\"utf-8\") as f:\n",
    "#             f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "# # Calculate total training time and print\n",
    "# total_time = time.time() - start_time\n",
    "# total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "# print('Training time {}'.format(total_time_str))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
