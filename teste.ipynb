{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device for training (e.g., 'cuda' or 'cpu')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data_path, transform=None):\n",
    "        self.data_path = data_path\n",
    "        self.transform = transform\n",
    "        self.file_list = [file for file in os.listdir(data_path) if file.endswith('.npy')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.file_list[idx]\n",
    "        file_path = os.path.join(self.data_path, file_name)\n",
    "        data = np.load(file_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import rotate  \n",
    "\n",
    "class ResizeNpyWithPadding:\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        h, w = data.shape\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        top = (new_h - h) // 2\n",
    "        bottom = new_h - h - top\n",
    "        left = (new_w - w) // 2\n",
    "        right = new_w - w - left\n",
    "        \n",
    "        resized_data = np.pad(data, ((top, bottom), (left, right)), mode='constant')\n",
    "        return resized_data\n",
    "\n",
    "# Custom transformation: Random Horizontal Flip for npy data\n",
    "class RandomHorizontalFlipNpy:\n",
    "    def __call__(self, data):\n",
    "        if np.random.random() < 0.5:\n",
    "            flipped_data = np.fliplr(data)\n",
    "            return flipped_data\n",
    "        return data\n",
    "\n",
    "class RandomRotationNpy:\n",
    "    def __init__(self, degrees):\n",
    "        self.degrees = degrees\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        angle = np.random.uniform(self.degrees[0], self.degrees[1])\n",
    "        rotated_data = rotate(data, angle, reshape=False, mode='constant', cval=0.0)\n",
    "        return rotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy.ndimage import rotate\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the class for resizing with padding\n",
    "class AddingPad:\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def __call__(self, data):\n",
    "        h, w = data.shape\n",
    "        \n",
    "        new_h, new_w = self.output_size\n",
    "        top = (new_h - h) // 2\n",
    "        bottom = new_h - h - top\n",
    "        left = (new_w - w) // 2\n",
    "        right = new_w - w - left\n",
    "        \n",
    "        padded_data = np.pad(data, ((top, bottom), (left, right)), mode='constant')\n",
    "        return padded_data\n",
    "    \n",
    "\n",
    "\n",
    "class ResizeNumpy:\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, np_array):\n",
    "        h, w = np_array.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        # Redimensionar o array NumPy usando a interpolação bilinear\n",
    "        resized_array = cv2.resize(np_array, (new_w, new_h), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        return resized_array\n",
    "\n",
    "\n",
    "\n",
    "# Define the class for random horizontal flipping\n",
    "class RandomHorizontalFlipNpy:\n",
    "    def __call__(self, data):\n",
    "        if np.random.rand() < 0.5:\n",
    "            data = np.fliplr(data)\n",
    "        return data\n",
    "\n",
    "# Define the class for random rotation\n",
    "class RandomRotationNpy:\n",
    "    def __init__(self, degrees):\n",
    "        self.degrees = degrees\n",
    "        \n",
    "    def __call__(self, data):\n",
    "        angle = np.random.uniform(self.degrees[0], self.degrees[1])\n",
    "        rotated_data = rotate(data, angle, reshape=False, mode='constant', cval=0.0)\n",
    "        return rotated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation\n",
    "transform_npy = transforms.Compose([\n",
    "    AddingPad((512,512)),\n",
    "    ResizeNumpy((128, 128)),\n",
    "    RandomHorizontalFlipNpy(),\n",
    "    RandomRotationNpy(degrees=(-30, 30)),  # Random rotation between -15 and 15 degrees\n",
    "    transforms.Lambda(lambda data: data.copy()),  # Copy the data to avoid grad error\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[130.10511327778235], std=[316.09062860899644])\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('D:/dados_tcc/train')\n",
    "\n",
    "\n",
    "lista_de_arquivos = [file for file in os.listdir(data_path) if file.endswith('.npy')]\n",
    "\n",
    "\n",
    "tranform_example = transform_npy(np.load(os.path.join(data_path, lista_de_arquivos[2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAJVElEQVR4nO3Ze4yU1RUA8HPO/R4zszP7ZBdkcReQXRClpaCIFUFt1GolmOIrVkvUFBtbazS+iiW1tk1aG2u0GHwWrVWbtIoV0FJtlSoKUvHBQxAR1lVgF/YxOzvzzffde8/pH7sao7PL8khMzff9MX9N5vzuuefee747eC98uQ99yfFjQAyIATEgBsSAGBADYkAMiAExIAbEgBgQA2JADIgBX0GAyIF93zmMoQEAAAEEvwSAAPSFFQESHDriMAAEQQBRhBFQHAcKrggKDI1wyABBBEDkUHlJpkhls7a5h5RFGloxHDIAQQDZug3B3p3KDMtPO3XqtWFdZ1Ll3SGl4FABAqCE3cpwSWru9Q3Ne/XjnU0VC6de3bJlRrsdSiEcKgCR8uj3rvGWbNS/3LD39DlHehW9Nz+4/DHd1jBKWdhvJeDB/WHRPzYB0GFD+uPkuu/MXn3P7WPd9vZuzWG9SdReOuMHid8nEHk/hoMC9FW4ACBg5ajnHp537El1p5x8azHfpcV1gSPjRRXTH9xx4TNe4MPgG8MBAgT61pcAoJAAy+tLnhjhLP3XjuHBmrwVEVTGILBje3DmX078zUWbCAEHERxgDfQtOWEXRDB0Tcpm65r9t6KgoyPXYYgIQVsgACOV/MwNl151+QUjCl7R4QEFQz4LREQEhFk0utVRkckOV0dsa3zqr6k7y7v2thU5m9dsrSElylFEkam9/6afHjFvyaht1YWBN4UhZkAQEQSFHNCVvdm1Uxv3BGWrmlpuf2LJmW9IZ1tnGBYN5hMZxI+K47XHopkx8+bVc15qVkt5+m410CwMJQMigsBRYCKJ2lp5XTg9d/W57a0f/Lb74eX0w/y0Sa/t7enKBgabxhcTazY8wGBNkM8XDbrlD/s/Ln/1zSdDAoGSWRhKBhDFklNVFth85qzjFjfdlqo+/d7c3G82jtSjNoz2XvlHslg0qJRe+MKL0tz4XoBBUDRMnu/wkWYP7aYddyweHZROweCrQAAA0FpM2/zbm488Z87Y5x85Y5zbdNmvL/j7zkRWV2aKHxVIQm3A9Z2KjclRaVtju6QYagZMuCkGQQVzNrS8ban0atzPMkQUAa8c3pWjVi7N7/zDEcHJs85fMKtnXwupkDyAYg66tLVMnk+UmbA5pzVWgDYi4ptMtpocLtPnjh5XNrPDK7kUBgYIAIo1qJzO13MbZ3w4D6q3FFatp7HdW9oZlY6iEE35IueKFtQMpAhNoVDmivIJhYiVbVq29BcZ5aUyqK+ZMrbLObAMCCIIpTLY0zu59Z+FR5rXbK+b9MJJuUI2UmC0EQm1cnWhNss5YQZgNuyIRcdFAXJdqlp/wsiWRCKTSKv2vxWqBtgPByxCEm29YNObPdOPf2jr5E2ZHI2O2sp2ZdnhINQCIDpy99RXhZvfmP+RFwEzM0SCDrERICEVfX2PcRNVlb5baH96VmaADJQGCIChinRn1WLT8ecrtnW0eF6nfodzOWWiYqiNFQAxtmfm3fdFN3fkVSRiLfc3pCyC5DoKscEm0zU1ZTWzw2WjflYVUSlBySkQBCmjDa9fef4te5rvbu+1VChERgAVa63ZWmNFWHPq3TtumFSoazUGrGUBEVBKgaDyU57j+spz6492TM31dsuYEEufCKUyIAjW2frxO/+e9mT21R3XYBgERltrgVC0YQBmFrGgoinbFxXNdjIs3LdiSSlCIsdzfE+5Kt3b01YzeWH3lafW+VK6BkpkQJAlnTvrqIlR9das1fmgGLG1xgCgWGsBgC0DgogqTGndJ2AMCzMIACpFREo5jpdMJnyR+jNmvTd664TpYbcdYCsuNQWsyt86e8e2zes7TG8u0GHBMLC1AsjMLIjCIoTCTDlSDMz9+QdFSpFS5Lhema+1NzJ5lT5mxna/A5yB9vwSUyDkPvv9cya8rFI2n9XWRFEEIMwiICICgEhsRcSySYhFACAQEAFUSimlHN93UXDa1OKjw031JlWR9/AATkMBtf72/9rXRpDXVWAbaWstgAiLQF90RBGwzALAoIQBgERQgAiRXNfRoVveWHHx87R2df0xiV7tCQ/Yl30BIKSTl1/YlU935AvdPlttET7tBgQREEAEECwLIpCgA8h9ckDXSypjxo2nD+877/hTNuVP41yPooHDlwCgqXjplFdbsezMxsz6FRQJMyD3ZxARQAQBUAEwMyA4ih1juU9FIZjE8NpFt52Zfv+62ikTwy5Q3uDvSJ8HCHGi84Pvdsm22SOmb0jme9Gylb7xAUl/Q4oIyhU2RhLKLw/2ie8mFOtisXF4tbf8W6/pV1admrI9eXJwkOwDQImGRDAYe8mu8oe+dtcDW2DdTp0LIiBShKgcIhRUjiIix/MSqXR5rVc1ovudsTUjR2VcSjWcsHbkipEtJ926jJO5XW1aobDg4G8GXyxCFYz+9Tq/onjTmIaVW4JsEthVAECCKIZd5ZM1TMpxMciFuckyLOeOzf7nxuHHqt0rbnj51taPKy+BnGbl9JXK/p7PA1DQlLW1BSsvu/K0uaPf3X726r3loBQCg1JGSFJkQiLPc0I7YkLFiU+WnbUv/auObP7auou+/ZyeOumOkaZNHESR/b8VAZTaiARN5cY/Vc/v+N5xY77BdS2rH/O1tukMRZGHYa1NOiaKQmsnlq+4637184kLdlQ9tXlByxXNOKxhmNMrEWDp7m+oABC06W2pip6dvbM/nKl/lJ+4fPOYqmxrUN8cZZteGQdv93rp+uqjj79xZTD5HufiCzH8yZ0rn80cRVFRs0LEIY59QAAIskeR4/ndFhK7di5a9uIfn1Y3rnriuvfPa9xwcvHx2Tue/t0Zt9TOz6c6U+Vbkx/sTYyorJUCE+HAO96BAEBQhFhYEQhWb6vcvfa0ucNX5x+dv3Be6qUF9WsvXfxMKzWNTxhHdMqmHCyaCBT2N94HckM0eFOKIggQJUVVFtsK/rB079ZRng8v9owZU5OQYt6iADJaEUL6zDXRYQEICvRdeREDaPSUaItl2oqt9IuBtkCKpP+1s2/sBxEdBu4J8ZPfQ0YAH9giKQiAUPVYRY77yZ4IcAjRBwF8liLIAAQiQCDIyuk7/D8lHmzsIQL6s9F/KYGAsL9Lj8MO6Fd8+nn4osNX8bI6BsSAGBADYkAMiAExIAbEgBgQA2JADIgBMSAGxIAYEAP+7wD/AyYM1BHuYgk/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Convert the tensor to a NumPy array for plotting\n",
    "image_to_plot = tranform_example.numpy()\n",
    "image_to_plot = image_to_plot.squeeze()  # Remove the channel dimension\n",
    "\n",
    "# Normalize the array to the range [0, 255] if needed\n",
    "image_to_plot = (image_to_plot * 255).astype(np.uint8)\n",
    "\n",
    "# Convert the NumPy array to a PIL Image\n",
    "pil_image = Image.fromarray(image_to_plot, mode='L')  # 'L' mode for grayscale\n",
    "\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699],\n",
       "        [-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699],\n",
       "        [-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699],\n",
       "        ...,\n",
       "        [-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699],\n",
       "        [-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699],\n",
       "        [-0.41160699, -0.41160699, -0.41160699, ..., -0.41160699,\n",
       "         -0.41160699, -0.41160699]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
